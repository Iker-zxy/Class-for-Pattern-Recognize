{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模式识别课程练习(lintcode)\n",
    "### 一.问题介绍\n",
    "\n",
    "[本题提供一个数据集, 它包括了5574条英文短信，每条短信内容由几个长短不一的句子组成。每条短信都标注好了是否为垃圾短信，通过该训练集训练出一个分类器，预测短信内容是否为垃圾短信。](https://www.lintcode.com/ai/spam-message-classification/overview)\n",
    "\n",
    "### 二.数据处理\n",
    "\n",
    "#### 1.读取数据\n",
    "\n",
    "对于题目中的csv格式的数据，使用[**`pandas`**](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)中的`pandas.read_csv`函数进行数据读取。cvs中的数据如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('./message/train.csv',names = ['Label','Text'])\n",
    "train = train.drop(0)\n",
    "test = pd.read_csv('./message/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "1   ham  Go until jurong point, crazy.. Available only ...\n",
       "2   ham                      Ok lar... Joking wif u oni...\n",
       "3  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "4   ham  U dun say so early hor... U c already then say...\n",
       "5   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head() # 训练集的cvs数据呈现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SmsId</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4456</td>\n",
       "      <td>Aight should I just plan to come up later toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>690</td>\n",
       "      <td>Was the farm open?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>944</td>\n",
       "      <td>I sent my scores to sophas and i had to do sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3768</td>\n",
       "      <td>Was gr8 to see that message. So when r u leavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1189</td>\n",
       "      <td>In that case I guess I'll see you at campus lodge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SmsId                                               Text\n",
       "0   4456  Aight should I just plan to come up later toni...\n",
       "1    690                                 Was the farm open?\n",
       "2    944  I sent my scores to sophas and i had to do sec...\n",
       "3   3768  Was gr8 to see that message. So when r u leavi...\n",
       "4   1189  In that case I guess I'll see you at campus lodge"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head() # 测试集的cvs数据呈现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于其Label,我们需要把ham定义成0,把spam定义成1,并把cvs的数据以数组的形式保存,程序如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "1      0  Go until jurong point, crazy.. Available only ...\n",
       "2      0                      Ok lar... Joking wif u oni...\n",
       "3      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "4      0  U dun say so early hor... U c already then say...\n",
       "5      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标签值转化\n",
    "class_mapping = {'ham':0, 'spam':1}\n",
    "train['Label'] = train['Label'].map(class_mapping)\n",
    "# 把其中的label和content分别保存，之后要对content进行处理\n",
    "train_label = train.Label.values\n",
    "train_data = train.Text.values\n",
    "test_data = test.Text.values\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.清洗数据\n",
    "\n",
    "初步的清洗数据，包括\n",
    "\n",
    "- 把所有的单词都转换成小写\n",
    "\n",
    "- 删除除了英文之外的字符\n",
    "\n",
    "- 把所有的单词恢复成词干\n",
    "\n",
    "  使用 [`nltk.stem.snowball module`](https://www.nltk.org/api/nltk.stem.html?highlight=nltk%20stem%20snowball#module-nltk.stem.snowball) 用于词干处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt') # 没有下载的话要先下载\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(comment_text):\n",
    "    comment_list = []\n",
    "    for text in comment_text:\n",
    "        # 将单词转换为小写\n",
    "        text = text.lower()\n",
    "        # 删除非字母、数字字符\n",
    "        text = re.sub(r\"[^a-z']\", \" \", text)\n",
    "        # 恢复常见的简写\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"can not \", text)\n",
    "        text = re.sub(r\"cannot\", \"can not \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"\\'m\", \" am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" will \", text)\n",
    "        text = re.sub(r\"ain\\'t\", \" are not \", text)\n",
    "        text = re.sub(r\"aren't\", \" are not \", text)\n",
    "        text = re.sub(r\"couldn\\'t\", \" can not \", text)\n",
    "        text = re.sub(r\"didn't\", \" do not \", text)\n",
    "        text = re.sub(r\"doesn't\", \" do not \", text)\n",
    "        text = re.sub(r\"don't\", \" do not \", text)\n",
    "        text = re.sub(r\"hadn't\", \" have not \", text)\n",
    "        text = re.sub(r\"hasn't\", \" have not \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        #进行词干提取\n",
    "        new_text = \"\"\n",
    "        s = nltk.stem.snowball.EnglishStemmer()  # 英文词干提取器\n",
    "        for word in word_tokenize(text):\n",
    "            new_text = new_text + \" \" + s.stem(word)\n",
    "        # 放回去\n",
    "        comment_list.append(new_text)\n",
    "    return comment_list\n",
    "\n",
    "train_data = clean_text(train_data)\n",
    "test_data = clean_text(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. TF-IDF计算\n",
    "\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率).\n",
    "\n",
    "> 是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，*用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度*。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n",
    "\n",
    "**上述引用总结就是, 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.**\n",
    "\n",
    "**词频 (term frequency, TF)** 指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否。）\n",
    "\n",
    "> 但是, 需要注意, 一些通用的词语对于主题并没有太大的作用, 反倒是一些出现频率较少的词才能够表达文章的主题, 所以单纯使用是TF不合适的。权重的设计必须满足：一个词预测主题的能力越强，权重越大，反之，权重越小。所有统计的文章中，一些词只是在其中很少几篇文章中出现，那么这样的词对文章的主题的作用很大，这些词的权重应该设计的较大。IDF就是在完成这样的工作.\n",
    "\n",
    "$$\n",
    "TF=\\frac{在某个文档中词条出现的次数}{该文档的所有词条数目}\n",
    "$$\n",
    "\n",
    "**逆向文件频率 (inverse document frequency, IDF)**的主要思想是：如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。\n",
    "$$\n",
    "IDF=log(\\frac{语料库的文档总数}{包含词条w的文档数+1}),分母之所以要加1，是为了避免分母为0\n",
    "$$\n",
    "**某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。**\n",
    "$$\n",
    "TF−IDF=TF∗IDF\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 数据的TF-IDF信息计算\n",
    "all_comment_list = list(train_data)+ list(test_data)\n",
    "text_vector = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode',token_pattern=r'\\w{1,}',\n",
    "                              max_features=5000, ngram_range=(1, 1), analyzer='word')\n",
    "text_vector.fit(all_comment_list)\n",
    "\n",
    "train_data = text_vector.transform(train_data)\n",
    "test_data = text_vector.transform(test_data)\n",
    "\n",
    "train_data = train_data.toarray()\n",
    "test_data = test_data.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三.模型选择\n",
    "\n",
    "采用Logistic Regreesion模型 from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "clf = LogisticRegression(C=100.0)\n",
    "clf.fit(train_data, train_label)\n",
    "train_acc = metrics.accuracy_score(clf.predict(train_data), train_label)\n",
    "print('train_acc:{}'.format(train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四.导出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测答案\n",
    "test_label = clf.predict_proba(test_data)\n",
    "answer = pd.read_csv(open(\"./message/Submission_LR_sklearn.csv\"))\n",
    "for i in range(test_label.shape[0]):\n",
    "    predit = test_label[i,0]\n",
    "    if predit < 0.5:\n",
    "        answer.loc[i,\"Label\"] = \"spam\"\n",
    "    else:\n",
    "        answer.loc[i,\"Label\"] = \"ham\"\n",
    "answer.to_csv(\"./message/Submission_LR_sklearn.csv\",index=False)  # 不要保存引索列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该导出结果在lintcode上已经可以达到1的准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附：下面是使用自己编写的LR程序\n",
    "⚠️ 训练过程的时间较久"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from scratch: 0.9949748743718593\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 定义sigmoid函数\n",
    "def sigmoid(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "# 定义似然函数\n",
    "def log_likelihood(features, target, weights):\n",
    "    scores = np.dot(features, weights)\n",
    "    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )\n",
    "    return ll\n",
    "\n",
    "# 定义线性回归模型\n",
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "        # Update weights with log likelihood gradient\n",
    "        output_error_signal = target - predictions\n",
    "        gradient = np.dot(features.T, output_error_signal)\n",
    "        weights =  weights + learning_rate * gradient\n",
    "\n",
    "#         # Print log-likelihood every so often\n",
    "#         if step % 10000 == 0:\n",
    "#             print(log_likelihood(features, target, weights))\n",
    "        \n",
    "    return weights\n",
    "\n",
    "# 计算准确性\n",
    "def accuracy(x,label,weights):\n",
    "    data_with_intercept = np.hstack((np.ones((x.shape[0], 1)),x))\n",
    "    final_scores = np.dot(data_with_intercept, weights)\n",
    "    preds = np.round(sigmoid(final_scores))\n",
    "    print ('Accuracy from scratch: {0}'.format((preds == label).sum().astype(float) / len(preds)))\n",
    "\n",
    "weights = logistic_regression(x_train, y_train,\n",
    "                     num_steps = 100000, learning_rate = 0.0001, add_intercept = True)\n",
    "\n",
    "train_acc = accuracy(train_data,train_label,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出结果\n",
    "data_with_intercept = np.hstack((np.ones((test_data.shape[0], 1)),test_data))\n",
    "final_scores = np.dot(data_with_intercept, weights)\n",
    "preds = np.round(sigmoid(final_scores))\n",
    "\n",
    "answer = pd.read_csv(open(\"./message/Submission_LR_scratch.csv\"))\n",
    "for i in range(answer.shape[0]):\n",
    "    predit = preds[i]\n",
    "    if predit ==1:\n",
    "        answer.loc[i,\"Label\"] = \"spam\"\n",
    "    else:\n",
    "        answer.loc[i,\"Label\"] = \"ham\"\n",
    "answer.to_csv(\"./message/Submission_LR_scratch.csv\",index=False)  # 不要保存引索列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在lintcode上的准确率可以达到 0.984753363"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
