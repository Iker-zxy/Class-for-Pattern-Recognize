###区分分类和回归问题

分类和回归的区别在于**输出变量的类型**。

我们常把连续变量预测问题归为回归问题，把离散变量预测归为分类问题。例如预测明天的气温是多少度，这是一个回归任务；预测明天是阴、晴还是雨，就是一个分类任务。在神经网络问题中，我们常考虑以下三种情况（其中）：

- **Regression：**输出一个标量 wx+b，这个值是连续值，所以可以用来处理**回归**问题

- **Binary classification：**把wx+b通过 sigmoid 函数映射到(0,1)上，并划分一个阈值，大于阈值的分为一类，小于等于分为另一类，可以用来处理**二分类**问题
- **Multi-class classification：**对于N分类问题，则是先得到N组w值不同的 wx+b，然后归一化，比如用 softmax 函数，最后变成N个类上的概率，可以处理**多分类**问题

### 输出层的激活函数和损失函数

**输出层的激活函数**和**损失函数**由任务类型决定，见下表。

| Task                       | Output layer activation | Loss function             | Express of loss function                                     |
| -------------------------- | ----------------------- | ------------------------- | ------------------------------------------------------------ |
| Regression                 | Linear                  | Mean squared error(MSE)   | $L(\boldsymbol{y},\boldsymbol{t})=||\boldsymbol{y}-\boldsymbol{t}||_{2}^{2}$ |
| Binary classification      | Sigmoid                 | Binary cross-entropy      | $L(\boldsymbol{y},\boldsymbol{t})=-\sum_{i=1}^{n}(1-t_{i})log(1-y_{i})$ |
| Multi-class classification | Softmax                 | Categorical cross-entropy | $L(\boldsymbol{y},\boldsymbol{t})=-\sum_{i=1}^{n}t_{i}logy_{i}$ |

### Softmax交叉熵损失函数求导公式

Softmax函数因其公式中分母为所有项之和，求导时非常不便，故特意单独拎出来进行推导，令
$$
z_{i}=\sum_{j}w_{ij}x_{ij}+b\\
y_{i}=\sigma(z_{i})=\frac{e^{z_{i}}}{\sum_{k}e^{z_{k}}}\\
$$
损失函数为
$$
J=-\sum_{i}t_{i}\ln y_{i}
$$
**推导过程:** 

根据链式法则，有
$$
\frac{\partial J}{\partial z_{i}}=\sum_{j}\frac{\partial J}{\partial y_{j}}\frac{\partial y_{j}}{\partial z_{i}}
$$
上述的公式中会有求和的形式，主要是由于softmax公式的特性，它的分母包含了所有神经元的输出，所以，对于不等于$i$的其他输出里面，也包含着$z_{i}$，所有的$y$都要纳入到计算范围中
$$
\frac{\partial J}{\partial y_{j}}=\frac{\partial(-\sum_{i}t_{i}\ln y_{i})}{\partial y_{j}}=-\frac{t_{j}}{y_{j}}
$$
对于$\frac{\partial y_{j}}{\partial z_{i}}​$,需要分为$i=j​$和$i≠j​$两种情况求导

1. 若$j = i$
   $$
   \frac{\partial y_{i}}{\partial z_{i}}=\frac{\partial \frac{e^{z_{i}}}{\sum_{k}e^{z_{k}}}}{\partial z_{i}} =\frac{e^{z_{i}}\sum_{k}e^{z_{k}}-(e^{z_{i}})^{2}}{(\sum_{k}e^{z_{k}})^{2}}=(\frac{e^{z_{i}}}{\sum_{k}e^{z_{k}}})(1-\frac{e^{z_{i}}}{\sum_{k}e^{z_{k}}})=y_{i}(1-y_{i})
   $$

2. 若$j\neq i$

$$
\frac{\partial y_{i}}{\partial z_{j}}=\frac{\partial \frac{e^{z_{j}}}{\sum_{k}e^{z_{k}}}}{\partial z_{i}} =-\frac{e^{z_{j}}e^{z_{i}}}{(\sum_{k}e^{z_{k}})^{2}}=-y_{i}y_{j}
$$

把两部分相结合，可以得到
$$
\frac{\partial J}{\partial z_{i}}=\sum_{j}(-\frac{t_{j}}{y_{j}})\frac{\partial y_{j}}{\partial z_{i}}=-\frac{t_{i}}{y_{i}}y_{i}(1-y_{i})+\sum_{j\neq i}\frac{t_{j}}{y_{j}}y_{i}y_{j}=-t_{i}+t_{i}y_{i}+\sum_{j\neq i}t_{j}y_{i}=-t_{i}+y_{i}\sum_{j}t_{j}
$$
对于分类问题，只有一个类别为1，其余都为0，所以$\sum_{j}t_{j}=1$

所以对于分类问题，有
$$
\frac{\partial J}{\partial z_{i}}=y_{i}-t_{i}
$$
参考资料

1.https://blog.csdn.net/qian99/article/details/78046329

2.