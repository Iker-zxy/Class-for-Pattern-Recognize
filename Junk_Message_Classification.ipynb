{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一.问题介绍\n",
    "\n",
    "[本题提供一个数据集, 它包括了5574条英文短信，每条短信内容由几个长短不一的句子组成。每条短信都标注好了是否为垃圾短信，通过该训练集训练出一个分类器，预测短信内容是否为垃圾短信。](https://www.lintcode.com/ai/spam-message-classification/overview)\n",
    "\n",
    "### 二.数据处理\n",
    "\n",
    "#### 1.读取数据\n",
    "\n",
    "对于题目中的csv格式的数据，使用[**`pandas`**](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)中的`pandas.read_csv`函数进行数据读取。cvs中的数据如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 读取数据\n",
    "train_data = pd.read_csv('../Temp/message/train.csv',names = ['Label','Text'])\n",
    "train_data = train_data.drop(0) # 数据都是字符，无法直接读取，自定name并把第一行删去\n",
    "test_data = pd.read_csv('../Temp/message/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "1   ham  Go until jurong point, crazy.. Available only ...\n",
       "2   ham                      Ok lar... Joking wif u oni...\n",
       "3  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "4   ham  U dun say so early hor... U c already then say...\n",
       "5   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head() # 训练集的cvs数据呈现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SmsId</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4456</td>\n",
       "      <td>Aight should I just plan to come up later toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>690</td>\n",
       "      <td>Was the farm open?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>944</td>\n",
       "      <td>I sent my scores to sophas and i had to do sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3768</td>\n",
       "      <td>Was gr8 to see that message. So when r u leavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1189</td>\n",
       "      <td>In that case I guess I'll see you at campus lodge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SmsId                                               Text\n",
       "0   4456  Aight should I just plan to come up later toni...\n",
       "1    690                                 Was the farm open?\n",
       "2    944  I sent my scores to sophas and i had to do sec...\n",
       "3   3768  Was gr8 to see that message. So when r u leavi...\n",
       "4   1189  In that case I guess I'll see you at campus lodge"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head() # 测试集的cvs数据呈现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "对于其Label，我们需要把ham定义成0，把spam定义成1，并把cvs的数据以数组的形式保存，程序如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_label = train_data.Label.values # dataframe -> array\n",
    "for i in range(train_data_label.shape[0]):\n",
    "    if train_data_label[i] == 'ham':\n",
    "        train_data_label[i] = 0\n",
    "    if train_data_label[i] == 'spam':\n",
    "        train_data_label[i] = 1\n",
    "        \n",
    "train_data_content = train_data.Text.values\n",
    "test_data_content = test_data.Text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "       'Ok lar... Joking wif u oni...',\n",
       "       \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "       ..., 'Pity, * was in mood for that. So...any other suggestions?',\n",
       "       \"The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free\",\n",
       "       'Rofl. Its true to its name'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Aight should I just plan to come up later tonight?',\n",
       "       'Was the farm open?',\n",
       "       'I sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one me the less expensive ones',\n",
       "       ...,\n",
       "       'I will be gentle princess! We will make sweet gentle love...',\n",
       "       'Beautiful Truth against Gravity.. Read carefully: \\\\Our heart feels light when someone is in it.. But it feels very heavy when someone leaves it..\\\\\" GOOD NIGHT\"',\n",
       "       'Ups which is 3days also, and the shipping company that takes 2wks. The other way is usps which takes a week but when it gets to lag you may have to bribe nipost to get your stuff.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.清洗数据\n",
    "\n",
    "初步的清洗数据，包括\n",
    "\n",
    "- 把所有的单词都转换成小写\n",
    "\n",
    "- 删除除了英文之外的字符\n",
    "\n",
    "- 把所有的单词恢复成词干\n",
    "\n",
    "  使用 [`nltk.stem.snowball module`](https://www.nltk.org/api/nltk.stem.html?highlight=nltk%20stem%20snowball#module-nltk.stem.snowball) 用于词干处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(comment_text):\n",
    "    comment_list = []\n",
    "    for text in comment_text:\n",
    "        # 将单词转换为小写\n",
    "        text = text.lower()\n",
    "        # 删除非字母、数字字符\n",
    "        text = re.sub(r\"[^a-z']\", \" \", text)\n",
    "        # 恢复常见的简写\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"can not \", text)\n",
    "        text = re.sub(r\"cannot\", \"can not \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"\\'m\", \" am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" will \", text)\n",
    "        text = re.sub(r\"ain\\'t\", \" are not \", text)\n",
    "        text = re.sub(r\"aren't\", \" are not \", text)\n",
    "        text = re.sub(r\"couldn\\'t\", \" can not \", text)\n",
    "        text = re.sub(r\"didn't\", \" do not \", text)\n",
    "        text = re.sub(r\"doesn't\", \" do not \", text)\n",
    "        text = re.sub(r\"don't\", \" do not \", text)\n",
    "        text = re.sub(r\"hadn't\", \" have not \", text)\n",
    "        text = re.sub(r\"hasn't\", \" have not \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        #进行词干提取\n",
    "        new_text = \"\"\n",
    "        s = nltk.stem.snowball.EnglishStemmer()  # 英文词干提取器\n",
    "        for word in word_tokenize(text):\n",
    "            new_text = new_text + \" \" + s.stem(word)\n",
    "        # 放回去\n",
    "        comment_list.append(new_text)\n",
    "    return comment_list\n",
    "\n",
    "train_data_content = clean_text(train_data_content)\n",
    "test_data_content = clean_text(test_data_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.训练集划分出验证集\n",
    "\n",
    "因为本题目的测试集中没有label，不便与我们自己验证算法的好坏，因此我们把训练集中70%的数据作为训练集，30%的数据作为验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3900 1670\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_data_content_length = len(train_data_content)\n",
    "train_data_label_length = len(train_data_label)\n",
    "train_length = int(train_data_content_length*0.7) \n",
    "train_data_content_for_train = train_data_content[0:train_length]  # 取70%的数据作为训练集\n",
    "train_data_content_for_test = train_data_content[(train_length+1):(train_data_label_length-1)]   # 取30%的数据作为验证集\n",
    "train_data_label_for_train = train_data_label[0:train_length]  # 取70%的数据作为训练集\n",
    "train_data_label_for_test = train_data_label[(train_length+1):(train_data_label_length-1)]   # 取30%的数据作为验证集\n",
    "train_data_label_for_train = train_data_label_for_train.astype(np.float64)\n",
    "train_data_label_for_test = train_data_label_for_test.astype(np.float64)\n",
    "print(len(train_data_content_for_train),len(train_data_content_for_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. TF-IDF计算\n",
    "\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率).\n",
    "\n",
    "> 是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，*用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度*。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n",
    "\n",
    "**上述引用总结就是, 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.**\n",
    "\n",
    "**词频 (term frequency, TF)** 指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否。）\n",
    "\n",
    "> 但是, 需要注意, 一些通用的词语对于主题并没有太大的作用, 反倒是一些出现频率较少的词才能够表达文章的主题, 所以单纯使用是TF不合适的。权重的设计必须满足：一个词预测主题的能力越强，权重越大，反之，权重越小。所有统计的文章中，一些词只是在其中很少几篇文章中出现，那么这样的词对文章的主题的作用很大，这些词的权重应该设计的较大。IDF就是在完成这样的工作.\n",
    "\n",
    "$$\n",
    "TF=\\frac{在某个文档中词条出现的次数}{该文档的所有词条数目}\n",
    "$$\n",
    "\n",
    "**逆向文件频率 (inverse document frequency, IDF)**的主要思想是：如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。\n",
    "$$\n",
    "IDF=log(\\frac{语料库的文档总数}{包含词条w的文档数+1}),分母之所以要加1，是为了避免分母为0\n",
    "$$\n",
    "**某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。**\n",
    "$$\n",
    "TF−IDF=TF∗IDF\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900, 5000) (1670, 5000) (1115, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 数据的TF-IDF信息计算\n",
    "all_comment_list = list(train_data_content_for_train)+list(train_data_content_for_test)+ list(test_data_content)\n",
    "text_vector = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode',token_pattern=r'\\w{1,}',\n",
    "                              max_features=5000, ngram_range=(1, 1), analyzer='word')\n",
    "text_vector.fit(all_comment_list)\n",
    "train_x_for_train = text_vector.transform(train_data_content_for_train)\n",
    "train_x_for_test = text_vector.transform(train_data_content_for_test)\n",
    "test_x = text_vector.transform(test_data_content)\n",
    "train_x_for_train = train_x_for_train.toarray()\n",
    "train_x_for_test = train_x_for_test.toarray()\n",
    "test_x = test_x.toarray()\n",
    "print(train_x_for_train.shape,train_x_for_test.shape,test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.07808484, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_for_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_for_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.12213731, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.07998202, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三.模型选择\n",
    "\n",
    "采用Logistic Regreesion模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义sigmoid函数\n",
    "def sigmoid(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "# 定义似然函数\n",
    "def log_likelihood(features, target, weights):\n",
    "    scores = np.dot(features, weights)\n",
    "    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )\n",
    "    return ll\n",
    "\n",
    "# 定义线性回归模型\n",
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "        # Update weights with log likelihood gradient\n",
    "        output_error_signal = target - predictions\n",
    "        gradient = np.dot(features.T, output_error_signal)\n",
    "        weights =  weights + learning_rate * gradient\n",
    "\n",
    "        # Print log-likelihood every so often\n",
    "        if step % 10000 == 0:\n",
    "            print(log_likelihood(features, target, weights))\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2502.687115328627\n",
      "-268.10768148676897\n",
      "-178.39304483589095\n",
      "-136.73831728675984\n",
      "-110.90951212733938\n",
      "-92.96626612346559\n",
      "-79.75737737969193\n",
      "-69.67402522675238\n",
      "-61.76162440326344\n",
      "-55.40590986963209\n"
     ]
    }
   ],
   "source": [
    "# 计算权重\n",
    "weights = logistic_regression(train_x_for_train, train_data_label_for_train,\n",
    "                     num_steps = 100000, learning_rate = 0.0001, add_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.9149225   3.76204305 -0.04741459 ... -0.01929058  1.01360014\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# 显示权重结果\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from scratch: 0.9994871794871795\n",
      "Accuracy from scratch: 0.9994871794871795\n"
     ]
    }
   ],
   "source": [
    "# 计算准确性\n",
    "def accuracy(x,label,weights):\n",
    "    data_with_intercept = np.hstack((np.ones((x.shape[0], 1)),x))\n",
    "    final_scores = np.dot(data_with_intercept, weights)\n",
    "    preds = np.round(sigmoid(final_scores))\n",
    "    print ('Accuracy from scratch: {0}'.format((preds == label).sum().astype(float) / len(preds)))\n",
    "    \n",
    "train = accuracy(train_x_for_train,train_data_label_for_train,weights)\n",
    "train_test = accuracy(train_x_for_train,train_data_label_for_train,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四.导出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_intercept = np.hstack((np.ones((test_x.shape[0], 1)),test_x))\n",
    "final_scores = np.dot(data_with_intercept, weights)\n",
    "preds = np.round(sigmoid(final_scores))\n",
    "\n",
    "answer = pd.read_csv(open(\"../Temp/message/Submission_LR.csv\"))\n",
    "for i in range(answer.shape[0]):\n",
    "    predit = preds[i]\n",
    "    if predit ==1:\n",
    "        answer.loc[i,\"Label\"] = \"spam\"\n",
    "    else:\n",
    "        answer.loc[i,\"Label\"] = \"ham\"\n",
    "answer.to_csv(\"../Temp/message/Submission_LR.csv\",index=False)  # 不要保存引索列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该导出结果在lintcode上已经可以达到0.99372的准确率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
